<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on Azhar Izzannada Elbachtiar</title><link>https://example.org/tags/ai/</link><description>Recent content in Ai on Azhar Izzannada Elbachtiar</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 11 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI, DevOps, and the Expanding Role of the Data Engineer</title><link>https://example.org/posts/2024_12_11_ai_devops_and_the_expanding_role_of_the_data_engineer/</link><pubDate>Wed, 11 Dec 2024 07:00:00 +0700</pubDate><guid>https://example.org/posts/2024_12_11_ai_devops_and_the_expanding_role_of_the_data_engineer/</guid><description>&lt;h1 id="blurring-the-lines-data-engineering-meets-devops-software-engineer-and-ml">Blurring the Lines: Data Engineering Meets DevOps, Software Engineer, and ML&lt;/h1>
&lt;p>In data engineering, your work often overlaps with DevOps, software engineering, and machine learning roles. Some companies hire a â€œfull-stackâ€ data engineer who wears all those hats. others split responsibilities into specialized teams. Personally, I love this variety. Itâ€™s an opportunity to pick up skills outside the typical data engineering toolkit. Most of these adjacent tasks arenâ€™t as deep or complex as the primary role, but mastering them can position you for broader opportunities. In fact, many data engineers evolve into Solutions Architects at larger organizations by demonstrating cross-functional expertise.&lt;/p></description></item><item><title>Data Mining Competition Earthquake Classification</title><link>https://example.org/portfolio/data_mining_competition_earthquake_classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/data_mining_competition_earthquake_classification/</guid><description>&lt;img src="https://example.org/images/data_mining_competition_earthquake_classification_hu_11f246b12e6af6c0.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>We were challenged to predict the phase of an earthquakeâ€”a multiclass classification taskâ€”within a strict &lt;strong>5-hour time limit&lt;/strong>. The results had to be submitted to Kaggle for leaderboard ranking and also presented live to a panel of judges.&lt;/p>
&lt;h1 id="resolve-and-result">Resolve and Result&lt;/h1>
&lt;p>We broke the process down into three key stages:&lt;/p>
&lt;ol>
&lt;li>Data Cleaning &amp;amp; Transformation&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Applied labeling and One-Hot Encoding to categorical features&lt;/li>
&lt;li>Split into train, validation, and test sets&lt;/li>
&lt;li>Scaling using Standard Scaler&lt;/li>
&lt;li>Handled class imbalance with SMOTE oversampling&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>Exploratory Data Analysis (EDA)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Used a heatmap to explore correlation between sensor values&lt;/li>
&lt;li>Discovered distribution of earthquake phases pretty different&lt;/li>
&lt;li>No missing values&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>Data Modeling&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Due to limited time and using &lt;strong>only a standard laptop&lt;/strong>, we prioritized fast experimentation&lt;/li>
&lt;li>Performed hyperparameter tuning with Randomized Search&lt;/li>
&lt;li>Tried several models:
&lt;ul>
&lt;li>Decision Tree&lt;/li>
&lt;li>Random Forest&lt;/li>
&lt;li>XGBoost (best-performing)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="-results">ðŸ† Results&lt;/h2>
&lt;ul>
&lt;li>Our XGBoost model achieved the highest accuracy of &lt;strong>90%&lt;/strong>&lt;/li>
&lt;li>We secured &lt;strong>1st place&lt;/strong> on the Kaggle leaderboard for the competition!&lt;/li>
&lt;/ul>








&lt;img src="https://example.org/images/data_mining_competition_earthquake_classification_1_hu_b0a0c115a5b69c46.png" alt="IMG_1">

&lt;h2 id="repository">Repository&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/azharizz/STEADataset_DataMining">&lt;strong>Github Code&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/competitions/finaldatamininganforcom20/leaderboard">&lt;strong>Kaggle&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Dimensional Modelling &amp; Modernize Data Warehouse</title><link>https://example.org/portfolio/dimensional_modelling_modernize_dw/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/dimensional_modelling_modernize_dw/</guid><description>&lt;img src="https://example.org/images/dimensional_modelling_modernize_dw_hu_3ba92601045b36e.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>Modernize and build a scalable Data Warehouse for Multiple Teams using a cloud-native. Key challenges include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Inconsistent data sources&lt;/strong>: Sales data spread across transactional systems, Firestore, and external feeds.&lt;/li>
&lt;li>&lt;strong>Lack of actionable insights&lt;/strong>: Difficulty measuring promotion ROI, territory performance, and customer segments in real time.&lt;/li>
&lt;li>&lt;strong>Complex ETL maintenance&lt;/strong>: Existing pipelines are brittle and hard to orchestrate.&lt;/li>
&lt;/ul>
&lt;h2 id="user-requirements">User Requirements&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Business Development Manager:&lt;/strong>
&lt;ul>
&lt;li>Analyze effectiveness of special offers and promotions on sales to inform future marketing decisions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Sales Manager:&lt;/strong>
&lt;ul>
&lt;li>Compare sales across territories and convert USDâ†”EUR to pinpoint top regions and assess currency impact.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Customer Relationship Manager:&lt;/strong>
&lt;ul>
&lt;li>Perform real-time clustering of customers by purchase frequency and average order value (since 2013) to tailor loyalty programs.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h1 id="result">Result&lt;/h1>
&lt;p>We designed a dimensional model in BigQuery and automated ELT/ETL with &lt;strong>dbt&lt;/strong>, &lt;strong>Airflow&lt;/strong>, and streaming components for real-time analytics.&lt;/p></description></item><item><title>NLP Brand Sentiment Analysis</title><link>https://example.org/portfolio/nlp_brand_sentiment_analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/nlp_brand_sentiment_analysis/</guid><description>&lt;img src="https://example.org/images/nlp_brand_sentiment_analysis_hu_aa99e2b48c0030fb.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>The Brand Department needed to identify &lt;strong>key factors&lt;/strong> affecting Customer Satisfaction and detect sudden &lt;strong>sentiment changes using NLP analysis on Twitter&lt;/strong>, in order to proactively prevent customer churn.&lt;/p>
&lt;h1 id="resolve-and-result">Resolve and Result&lt;/h1>
&lt;ol>
&lt;li>Data Collection:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Data collected with python library using list relevant keyword.&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>Data Cleaning:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Remove Official Acc, Duplicate Tweet, Promotion Tweet&lt;/li>
&lt;li>Remove Spam or Undetected Duplicate&lt;/li>
&lt;li>Remove Buzzer Tweet&lt;/li>
&lt;li>Remove Unnecessary characters (Number, Emoji, Mention, etc.)&lt;/li>
&lt;li>Stopword Removal&lt;/li>
&lt;li>Stemming word&lt;/li>
&lt;li>Slang Words changer&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>EDA (Exploratory Data Analysis):&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Categorization based on keyword&lt;/li>
&lt;li>Sudden changes by Customer (Positive to negative or otherwise)&lt;/li>
&lt;li>Getting additional tweet location by text&lt;/li>
&lt;li>Hashtag analysis&lt;/li>
&lt;/ul>
&lt;ol start="4">
&lt;li>Data Modeling:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Labelling Data using Fleiss&amp;rsquo; Kappa rules&lt;/li>
&lt;li>Train, Test, Validation split (0.7, 0.2, 0.1)&lt;/li>
&lt;li>Tokenization and building vocabulary&lt;/li>
&lt;li>Model creation using &lt;strong>Bidirectional LSTM (Pytorch)&lt;/strong>&lt;/li>
&lt;li>Data Training and Hyperparameter Optimization (Gridsearch)&lt;/li>
&lt;li>Evaluation and Visualization&lt;/li>
&lt;/ul>
&lt;h2 id="-results">ðŸ† Results&lt;/h2>
&lt;p>The model got an f1-score with &lt;strong>92%&lt;/strong> accuracy, Train loss is &lt;strong>0.035&lt;/strong>, and validation loss is &lt;strong>0.020&lt;/strong>. It means the model I create has a great result to be &lt;strong>deployed&lt;/strong>.&lt;/p></description></item><item><title>Product Cataloging Tokopedia</title><link>https://example.org/portfolio/product_cataloging_tokopedia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/product_cataloging_tokopedia/</guid><description>&lt;img src="https://example.org/images/product_cataloging_hu_f9fd212aaa85dda7.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>This project is a replication of a real Tokopedia project presented at the &lt;strong>START Summit Extension on February 24, 2022&lt;/strong>, titled &lt;strong>&amp;ldquo;How We Leverage AI to Match Products.&amp;rdquo;&lt;/strong> It aims to improve the customer buying experience, which can sometimes be challengingâ€”for example, searching for &amp;ldquo;KF 95 Masker&amp;rdquo; may return results where only 8 out of 10 products are actually related to KF 95, while the rest are KF94.&lt;/p>
&lt;h1 id="result">Result&lt;/h1>
&lt;p>Before starting the product cataloging process, we cleaned and transformed the data with the following steps:&lt;/p></description></item><item><title>Realtime Car Pricing Prediction</title><link>https://example.org/portfolio/realtime_car_pricing_prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/realtime_car_pricing_prediction/</guid><description>&lt;img src="https://example.org/images/realtime_car_pricing_prediction_hu_5f833a8c2aee2df0.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>This project tackles the challenge of real-time car price prediction in used car marketplaces, where pricing is often &lt;strong>inconsistent and subjective&lt;/strong>. The goal is to provide an AI-powered tool that predicts fair market prices instantly, helping both sellers and buyers make better decisions&lt;/p>
&lt;h1 id="result">Result&lt;/h1>
&lt;p>Tools Used:&lt;/p>
&lt;ul>
&lt;li>MySQL&lt;/li>
&lt;li>Debezium&lt;/li>
&lt;li>Kafka&lt;/li>
&lt;li>Spark&lt;/li>
&lt;li>FastAPI&lt;/li>
&lt;li>GCP&lt;/li>
&lt;li>Cloud Storage&lt;/li>
&lt;li>Cloud Functions&lt;/li>
&lt;li>BigQuery&lt;/li>
&lt;li>ML Model&lt;/li>
&lt;li>Docker&lt;/li>
&lt;/ul>








&lt;img src="https://example.org/images/realtime_car_pricing_prediction_1_hu_48ee5c3f1dcf2ebe.png" alt="IMG_1">

&lt;p>Using the tools above, we built the data infrastructure and pipeline from &lt;strong>scratch&lt;/strong>, enabling seamless real-time price prediction. The system streams data from MySQL using Debezium, processes it with Spark, and serves predictions through a &lt;strong>FastAPI endpoint&lt;/strong>.&lt;/p></description></item><item><title>Realtime Water Quality Prediction End-to-End Pipeline</title><link>https://example.org/portfolio/realtime_water_quality_prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/realtime_water_quality_prediction/</guid><description>&lt;img src="https://example.org/images/realtime_water_quality_prediction_hu_32263fecdafdacbe.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>Monitoring water quality in real time is crucial for public health, industrial operations, and environmental safety. Traditional systems often rely on delayed batch data, which makes it hard to respond quickly to dangerous conditions such as pH spikes and contamination that will affect in example the business pond fish.&lt;/p>
&lt;p>The challenge: &lt;strong>build a scalable, real-time water quality prediction pipeline&lt;/strong> that processes sensor data as itâ€™s collected, predicts quality metrics instantly, and stores validated results for downstream analytics.&lt;/p></description></item></channel></rss>