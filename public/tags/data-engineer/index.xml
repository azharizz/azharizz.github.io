<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Engineer on Azhar Izzannada Elbachtiar</title><link>https://example.org/tags/data-engineer/</link><description>Recent content in Data Engineer on Azhar Izzannada Elbachtiar</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 11 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/tags/data-engineer/index.xml" rel="self" type="application/rss+xml"/><item><title>AI, DevOps, and the Expanding Role of the Data Engineer</title><link>https://example.org/posts/2024_12_11_ai_devops_and_the_expanding_role_of_the_data_engineer/</link><pubDate>Wed, 11 Dec 2024 07:00:00 +0700</pubDate><guid>https://example.org/posts/2024_12_11_ai_devops_and_the_expanding_role_of_the_data_engineer/</guid><description>&lt;h1 id="blurring-the-lines-data-engineering-meets-devops-software-engineer-and-ml">Blurring the Lines: Data Engineering Meets DevOps, Software Engineer, and ML&lt;/h1>
&lt;p>In data engineering, your work often overlaps with DevOps, software engineering, and machine learning roles. Some companies hire a “full-stack” data engineer who wears all those hats. others split responsibilities into specialized teams. Personally, I love this variety. It’s an opportunity to pick up skills outside the typical data engineering toolkit. Most of these adjacent tasks aren’t as deep or complex as the primary role, but mastering them can position you for broader opportunities. In fact, many data engineers evolve into Solutions Architects at larger organizations by demonstrating cross-functional expertise.&lt;/p></description></item><item><title>The Unwritten Rules of Data Engineering Tools Job Requirements</title><link>https://example.org/posts/2024_09_17_the_unwritten_rules_of_data_engineering_tools_job_requirements/</link><pubDate>Tue, 17 Sep 2024 07:00:00 +0700</pubDate><guid>https://example.org/posts/2024_09_17_the_unwritten_rules_of_data_engineering_tools_job_requirements/</guid><description>&lt;img src="https://example.org/images/A_hu_b8e5f132d2d60c3e.png" alt="A_Post">

&lt;h1 id="the-tool-overload-in-data-engineering">The Tool Overload in Data Engineering&lt;/h1>
&lt;p>Data engineering can feel like a never-ending buffet of tools like ETL frameworks, data warehouses, orchestration platforms, container services, and more. Some people even joke that data engineers are really “tool operators”. While each tool can simplify a specific task, juggling too many often makes workflows more complex than consolidating around a handful of versatile solutions. There’s no perfect fit toolset, but identifying the handful of platforms that match your daily workload is key to staying efficient and sane.&lt;/p></description></item><item><title>Spot Instances Cost and Performance Saviour on Short Lived Task</title><link>https://example.org/posts/2024_05_01_spot_instances_cost_and_performance_saviour_on_short_lived_task/</link><pubDate>Wed, 01 May 2024 07:00:00 +0700</pubDate><guid>https://example.org/posts/2024_05_01_spot_instances_cost_and_performance_saviour_on_short_lived_task/</guid><description>&lt;h1 id="unlocking-the-power-of-spot-instances-for-short-lived-tasks">Unlocking the Power of Spot Instances for Short-Lived Tasks &lt;/h1>
&lt;p>Many people assume Spot Instances are only useful for testing, but they can be a game-changer when applied strategically. If your workload can tolerate interruptions or finishes well within the typical 24-hour eviction window Spot Instances become an incredibly cost-effective option. In my experience, any job that completes in under six hours is a perfect candidate for Spot. You’ll get the exact same performance as a standard on-demand instance, but at a fraction of the cost.&lt;/p></description></item><item><title>Data Engineer Portfolio Creation Things</title><link>https://example.org/posts/2023_12_31_data_engineer_portfolio_creation_things/</link><pubDate>Sun, 31 Dec 2023 01:01:01 +0700</pubDate><guid>https://example.org/posts/2023_12_31_data_engineer_portfolio_creation_things/</guid><description>&lt;h1 id="learning-by-doing">Learning by Doing&lt;/h1>
&lt;p>The principle of learning can take many forms—through different media, structured steps, supportive environments, or any combination that makes the process enjoyable, fun, easy to understand, relaxed, and focused, thereby adding positive value. &lt;strong>Learning by Doing&lt;/strong> is one such approach. Personally, I believe that &lt;strong>mastering the fundamentals&lt;/strong> is essential, but when you’re a programmer or working in a highly technical field, &lt;strong>hands‑on experience&lt;/strong> is crucial for transforming foundational theory into real‑world implementations.&lt;/p></description></item><item><title>Dimensional Modelling &amp; Modernize Data Warehouse</title><link>https://example.org/portfolio/dimensional_modelling_modernize_dw/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/dimensional_modelling_modernize_dw/</guid><description>&lt;img src="https://example.org/images/dimensional_modelling_modernize_dw_hu_3ba92601045b36e.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>Modernize and build a scalable Data Warehouse for Multiple Teams using a cloud-native. Key challenges include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Inconsistent data sources&lt;/strong>: Sales data spread across transactional systems, Firestore, and external feeds.&lt;/li>
&lt;li>&lt;strong>Lack of actionable insights&lt;/strong>: Difficulty measuring promotion ROI, territory performance, and customer segments in real time.&lt;/li>
&lt;li>&lt;strong>Complex ETL maintenance&lt;/strong>: Existing pipelines are brittle and hard to orchestrate.&lt;/li>
&lt;/ul>
&lt;h2 id="user-requirements">User Requirements&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Business Development Manager:&lt;/strong>
&lt;ul>
&lt;li>Analyze effectiveness of special offers and promotions on sales to inform future marketing decisions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Sales Manager:&lt;/strong>
&lt;ul>
&lt;li>Compare sales across territories and convert USD↔EUR to pinpoint top regions and assess currency impact.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Customer Relationship Manager:&lt;/strong>
&lt;ul>
&lt;li>Perform real-time clustering of customers by purchase frequency and average order value (since 2013) to tailor loyalty programs.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h1 id="result">Result&lt;/h1>
&lt;p>We designed a dimensional model in BigQuery and automated ELT/ETL with &lt;strong>dbt&lt;/strong>, &lt;strong>Airflow&lt;/strong>, and streaming components for real-time analytics.&lt;/p></description></item><item><title>Go CLI App Kafka Retention Policy Manager</title><link>https://example.org/portfolio/go_cli_app_kafka_retention_policy_manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/go_cli_app_kafka_retention_policy_manager/</guid><description>&lt;img src="https://example.org/images/go_cli_app_kafka_retention_hu_cd086d2f9d6d4972.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>Managing Kafka topic retention policies at scale is manual, error-prone, and carries two main risks:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Data Loss&lt;/strong>: Misconfigured retention can delete critical streaming data.&lt;/li>
&lt;li>&lt;strong>Cost Overruns&lt;/strong>: Unvalidated data retention leads to inflated storage bills.&lt;/li>
&lt;/ul>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;p>To solve the problem, one approach is to create app like CLI to automate easily with the features:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Automated Retention Updates&lt;/strong>&lt;br>
Applies bulk topic settings via Kafka’s Admin API in one command.&lt;/li>
&lt;li>&lt;strong>Integrity Validation&lt;/strong>
&lt;ul>
&lt;li>Stores expected message counts in Redis&lt;/li>
&lt;li>Compares Redis counts with files or stored counts in GCS&lt;/li>
&lt;li>Halts on discrepancies to prevent data loss&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Cost-Efficient Archiving&lt;/strong>
&lt;ul>
&lt;li>Moves validated data from Cloud Storage to Coldline/Archive tiers&lt;/li>
&lt;li>Optionally deletes stale data post-archive&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Easy Integration&lt;/strong>&lt;br>
Docker-packaged CLI, fits into CI/CD pipelines or scheduled jobs.&lt;/li>
&lt;/ul>
&lt;p>Stack Used:&lt;/p></description></item><item><title>Realtime Car Pricing Prediction</title><link>https://example.org/portfolio/realtime_car_pricing_prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/realtime_car_pricing_prediction/</guid><description>&lt;img src="https://example.org/images/realtime_car_pricing_prediction_hu_5f833a8c2aee2df0.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>This project tackles the challenge of real-time car price prediction in used car marketplaces, where pricing is often &lt;strong>inconsistent and subjective&lt;/strong>. The goal is to provide an AI-powered tool that predicts fair market prices instantly, helping both sellers and buyers make better decisions&lt;/p>
&lt;h1 id="result">Result&lt;/h1>
&lt;p>Tools Used:&lt;/p>
&lt;ul>
&lt;li>MySQL&lt;/li>
&lt;li>Debezium&lt;/li>
&lt;li>Kafka&lt;/li>
&lt;li>Spark&lt;/li>
&lt;li>FastAPI&lt;/li>
&lt;li>GCP&lt;/li>
&lt;li>Cloud Storage&lt;/li>
&lt;li>Cloud Functions&lt;/li>
&lt;li>BigQuery&lt;/li>
&lt;li>ML Model&lt;/li>
&lt;li>Docker&lt;/li>
&lt;/ul>








&lt;img src="https://example.org/images/realtime_car_pricing_prediction_1_hu_48ee5c3f1dcf2ebe.png" alt="IMG_1">

&lt;p>Using the tools above, we built the data infrastructure and pipeline from &lt;strong>scratch&lt;/strong>, enabling seamless real-time price prediction. The system streams data from MySQL using Debezium, processes it with Spark, and serves predictions through a &lt;strong>FastAPI endpoint&lt;/strong>.&lt;/p></description></item><item><title>Realtime Water Quality Prediction End-to-End Pipeline</title><link>https://example.org/portfolio/realtime_water_quality_prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/portfolio/realtime_water_quality_prediction/</guid><description>&lt;img src="https://example.org/images/realtime_water_quality_prediction_hu_32263fecdafdacbe.gif" alt="GIF_Page">

&lt;h1 id="problem">Problem&lt;/h1>
&lt;p>Monitoring water quality in real time is crucial for public health, industrial operations, and environmental safety. Traditional systems often rely on delayed batch data, which makes it hard to respond quickly to dangerous conditions such as pH spikes and contamination that will affect in example the business pond fish.&lt;/p>
&lt;p>The challenge: &lt;strong>build a scalable, real-time water quality prediction pipeline&lt;/strong> that processes sensor data as it’s collected, predicts quality metrics instantly, and stores validated results for downstream analytics.&lt;/p></description></item></channel></rss>